
## Setup and Installation

1.  **Clone the Repository:**
    ```bash
    git clone <repository-url>
    cd nq_orb_ml_enhancement
    ```
2.  **Place Data:**
    *   Put all your raw 1-minute Nasdaq futures data files (e.g., `NQ 03-15.Last.txt`, `NQ 06-15.Last.txt`, ...) into the `data/raw/` directory.
    *   Place the trade log CSV file generated by your original NinjaTrader strategy into the `data/raw/` directory. **Ensure the filename matches** the `trade_log_file` setting in `config/config.yaml`.
3.  **Create Virtual Environment (Recommended):**
    ```bash
    python -m venv venv
    # Activate (Windows Powershell/Cmd):
    .\venv\Scripts\activate
    # Activate (Git Bash on Windows):
    source venv/Scripts/activate
    # Activate (macOS/Linux):
    source venv/bin/activate
    ```
    *Note: This project was developed encountering issues with Python 3.13 library compatibility (specifically `pandas-ta` and `numpy`). Using Python 3.11 or 3.12 might provide a smoother experience.*
4.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *Troubleshooting:* If you encounter installation errors (especially for `numpy`, `pandas`, or related build issues on Windows), refer to common solutions like upgrading pip (`pip install --upgrade pip setuptools wheel`), installing C++ Build Tools for Visual Studio, or manually installing specific package wheels (`.whl` files).
5.  **Enable Jupyter Widgets (if needed):** If progress bars in notebooks don't display correctly after installing requirements:
    ```bash
    pip install ipywidgets
    jupyter nbextension enable --py widgetsnbextension --sys-prefix
    # Restart VS Code or your Jupyter environment after this.
    ```
6.  **Configure:** Review and adjust parameters in `config/config.yaml`, especially file paths, date splits, feature parameters, and hyperparameter tuning settings (`n_trials`).

## Usage / Workflow

The project is designed to be run sequentially through the Jupyter notebooks located in the `notebooks/` directory. **Run the notebooks in order (01 through 06)**, as each step depends on the output of the previous one.

1.  **`01_Data_Exploration_and_Cleaning.ipynb`**: Loads raw price data (multiple files) and the trade log. Cleans data, handles timestamps, explores basic properties, and saves the combined, cleaned price data to `data/processed/`.
2.  **`02_Feature_Engineering.ipynb`**: Loads the processed price data. Calculates all technical indicators, ORB/daily context features, time features, and interaction features based on `config.yaml`. Creates lagged versions (`_lag1`) required for modeling. Saves the final feature-rich dataframe to `data/processed/`.
3.  **`03_Regime_Detection.ipynb`**: Loads the features data. Trains a K-Means clustering model on the specified regime features (`config.yaml`) using the training data split. Saves the regime model and scaler to `models/`. Predicts regimes for the entire dataset and adds the 'Regime' column. Saves the updated dataframe. Analyze the printed cluster centers to understand regime meanings and **update `position_sizing_params.regime_multipliers` in `config.yaml`**.
4.  **`04_Meta_Labeling_Model_Training.ipynb`**: Loads features data (with regimes) and the original trade log. Aligns features available before each trade with the trade's Win/Loss outcome. Performs hyperparameter tuning for LightGBM using Optuna and TimeSeriesSplit on the training data (if enabled in `config.yaml`). Trains the final LightGBM model using the best found (or default) parameters on the combined train+validation set. Saves the meta-labeling model and scaler to `models/`. Evaluates the final model on the hold-out test set.
5.  **`05_Position_Sizing_Logic.ipynb`**: Tests and visualizes the position sizing logic defined in `src/position_sizing.py` using the parameters from `config.yaml` (confidence threshold, regime multipliers) to ensure it behaves as expected.
6.  **`06_Backtesting_Simulation.ipynb`**: Initializes the `OrbMlSimulator` class from `src/simulation.py`. Loads the feature data and trained models/scalers. Runs a bar-by-bar simulation of the ORB strategy over the specified period (typically validation + test sets). For each potential ORB signal, it gets predictions from the loaded models (regime, win probability), applies the position sizing logic, and simulates trade execution. Saves the simulated trade log and performance metrics to the `results/` directory. Optionally compares results against the original strategy's performance over the same period.

## Core Components (`src/`)

*   **`data_processing.py`**: Functions for loading and cleaning raw price data (multiple files) and the trade log CSV (handling currency formats).
*   **`feature_engineering.py`**: Calculates technical indicators (`pandas-ta`), ORB/daily context, time features, interactions, and lags them. Driven by `config.yaml`.
*   **`regime_model.py`**: Functions for training (K-Means) and predicting market regimes. Saves/loads model and scaler.
*   **`meta_labeling_model.py`**: Functions for training (LightGBM with imbalance handling) and predicting trade outcome probability. Saves/loads model and scaler. Handles retraining logic with best parameters.
*   **`position_sizing.py`**: Function defining position sizing logic based on confidence, regime, and base size.
*   **`simulation.py`**: Contains the `OrbMlSimulator` class, which orchestrates the bar-by-bar backtest, calling ML models for predictions and applying strategy/sizing rules.
*   **`utils.py`**: Helper functions (e.g., calculating performance metrics from a trade log).

## Configuration (`config.yaml`)

All major parameters are controlled via `config/config.yaml`:

*   File paths and names.
*   Data splitting dates (Train/Validation/Test).
*   Original ORB strategy parameters (used for simulation logic and feature calculation).
*   Feature engineering parameters (indicator periods).
*   Regime model settings (number of regimes, features used for clustering).
*   Hyperparameter tuning settings (enable/disable, number of trials, metric).
*   Meta-labeling model settings (features list, probability threshold).
*   Position sizing settings (method, confidence threshold, regime multipliers).
*   Simulation settings (commission, slippage).

## Potential Future Work

*   **Advanced Feature Engineering:** Explore order flow features (if data available), market breadth, cross-asset correlations, higher timeframe features.
*   **Refined Target Variable:** Instead of binary Win/Loss, predict R:R achieved, or probability of hitting TP before SL.
*   **Feature Selection:** Implement automated feature selection (e.g., RFE, Boruta) after tuning.
*   **Alternative Models:** Test different classification algorithms (Random Forest, SVM, Logistic Regression) or regression models if changing the target.
*   **Walk-Forward Optimization:** Implement a more rigorous walk-forward backtesting framework instead of a single Train/Val/Test split.
*   **Live Integration:** Develop the recommended Python API service and C# client modifications for real-time deployment in NinjaTrader.
*   **Improved Simulation:** Enhance `simulation.py` to handle commissions, slippage, and intra-bar SL/TP logic more realistically.

## Disclaimer

This project is for educational and research purposes only. Trading financial instruments involves substantial risk of loss. Past performance is not indicative of future results. The strategies, models, and code provided here are not investment advice. Any decisions made based on this project are at your own risk. Backtested results may differ significantly from live trading results due to factors like slippage, commissions, data feed differences, and model degradation. Use caution and conduct thorough testing before deploying any trading strategy live.